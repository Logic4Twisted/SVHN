{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import tarfile\n",
    "from IPython.display import display, Image\n",
    "from scipy import ndimage, misc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from six.moves.urllib.request import urlretrieve\n",
    "from six.moves import cPickle as pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = '/home/ubuntu/pynb/SVHN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/pynb/SVHN/data/train already present - Skipping extraction of /home/ubuntu/pynb/SVHN/data/train.tar.gz.\n",
      "/home/ubuntu/pynb/SVHN/data/test already present - Skipping extraction of /home/ubuntu/pynb/SVHN/data/test.tar.gz.\n"
     ]
    }
   ],
   "source": [
    "def maybe_extract(filename, force=False):\n",
    "  root = os.path.splitext(os.path.splitext(filename)[0])[0]  # remove .tar.gz\n",
    "  if os.path.isdir(root) and not force:\n",
    "    # You may override by setting force=True.\n",
    "    print('%s already present - Skipping extraction of %s.' % (root, filename))\n",
    "  else:\n",
    "    print('Extracting data for %s. This may take a while. Please wait.' % root)\n",
    "    tar = tarfile.open(filename)\n",
    "    sys.stdout.flush()\n",
    "    tar.extractall(path = root)\n",
    "    tar.close()\n",
    "\n",
    "maybe_extract(root_dir + '/data/train.tar.gz')\n",
    "maybe_extract(root_dir + '/data/test.tar.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: coverting from .mat to .csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read labels from csv files. How many examples do we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/pynb/SVHN/data/train/trainData.csv\n",
      "Training dataset size  33402  with  73258  digits.\n",
      "Test datasset size  13068  with  26033  digits.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "def read_csv(filename):\n",
    "    with open(filename , 'rt') as f:\n",
    "        reader = csv.reader(f)\n",
    "        data_list = list(reader)\n",
    "        \n",
    "        d = {}\n",
    "        for item in data_list:\n",
    "            if not item[1].isdigit():\n",
    "                continue\n",
    "            if not item[0] in d:\n",
    "                d[item[0]] = []\n",
    "            d[item[0]].append([int(x) for x in item[1:]])\n",
    "        return d, len(data_list)\n",
    "    \n",
    "def count_examples_by_digits(labels_dict):\n",
    "    count = {1:0, 2:0, 3:0, 4:0, 5:0, 6:0}\n",
    "    for x in labels_dict:\n",
    "        count[len(labels_dict[x])] = count[len(labels_dict[x])] + 1\n",
    "    count = {c: 100.0*count[c]/len(labels_dict) for c in count}\n",
    "    return count\n",
    "    \n",
    "print (root_dir + '/data/train/trainData.csv')\n",
    "labels_train_data_dict, training_digits = read_csv(root_dir + '/data/train/trainData.csv')\n",
    "labels_test_data_dict, test_digits = read_csv(root_dir + '/data/test/testData.csv')\n",
    "print ('Training dataset size ', len(labels_train_data_dict), ' with ',training_digits, ' digits.')\n",
    "print ('Test datasset size ', len(labels_test_data_dict), ' with ',test_digits, ' digits.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the distribution of examples by number of digits in them?\n",
    "First for the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  digit 15.379318603676426 %\n",
      "2  digit 54.278186934914075 %\n",
      "3  digit 26.019400035925994 %\n",
      "4  digit 4.293156098437219 %\n",
      "5  digit 0.02694449434165619 %\n",
      "6  digit 0.0029938327046284655 %\n"
     ]
    }
   ],
   "source": [
    "count_training = count_examples_by_digits(labels_train_data_dict)\n",
    "for key in count_training:\n",
    "    print (key, ' digit', count_training[key], '%')\n",
    "plt.title('Examples(%) per number of digits in training set')\n",
    "plt.xlabel('Number of digits')\n",
    "plt.ylabel('Examples (%)')\n",
    "plt.xticks(range(len(count_training)), count_training.keys())\n",
    "plt.bar(range(len(count_training)), count_training.values(), align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that there are more than 50% two digit examples, and there are very few examples with more than 4 digits.\n",
    "Let's plot the same graph for test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  digit 19.000612182430363 %\n",
      "2  digit 63.94245485154576 %\n",
      "3  digit 15.924395469850015 %\n",
      "4  digit 1.1172329354147537 %\n",
      "5  digit 0.015304560759106214 %\n",
      "6  digit 0.0 %\n"
     ]
    }
   ],
   "source": [
    "count_test = count_examples_by_digits(labels_test_data_dict)\n",
    "for key in count_test:\n",
    "    print (key, ' digit', count_test[key], '%')\n",
    "plt.title('Examples(%) per number of digits in test set')\n",
    "plt.xlabel('Number of digits')\n",
    "plt.ylabel('Examples(%)')\n",
    "plt.xticks(range(len(count_test)), count_test.keys())\n",
    "plt.bar(range(len(count_training)), count_test.values(), align='center')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set see that distribution is similar, with slightly more two digit numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAFMAAAAiCAIAAACsrnIzAAAAB3RJTUUH2wsYByc4odgUUwAAEU5J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\nQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[[9, 41, 6, 12, 22], [7, 30, 7, 10, 22]]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "display(Image(filename= root_dir + '/data/train/10855.png'))\n",
    "labels_train_data_dict['10855.png']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape: (83, 167, 3)\n",
      "[1, 10]\n",
      "[[1, 48, 9, 32, 67], [10, 82, 13, 33, 67]]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "\n",
    "to_display = '13068.png'\n",
    "\n",
    "im = np.array(Image.open(root_dir + '/data/train/' + to_display), dtype=np.uint8)\n",
    "# Create figure and axes\n",
    "fig,ax = plt.subplots(1)\n",
    "# Display the image\n",
    "ax.imshow(im)\n",
    "print ('image shape:', im.shape)\n",
    "for digit_loc in labels_train_data_dict[to_display]:\n",
    "    # Create a Rectangle patch\n",
    "    rect = patches.Rectangle((digit_loc[1],digit_loc[2]),digit_loc[3],digit_loc[4],linewidth=1,edgecolor='r',facecolor='none')\n",
    "    # Add the patch to the Axes\n",
    "    ax.add_patch(rect)\n",
    "labels_train_data_dict[to_display].sort(key=lambda x: x[1])\n",
    "print ([d[0] for d in labels_train_data_dict[to_display]])\n",
    "print (labels_train_data_dict[to_display])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function calculates minimum rectangle that contains all digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_rectangle_containing_all_digits(digits):\n",
    "    x = max(0, min([d[1] for d in digits]))\n",
    "    y = max(0, min([d[2] for d in digits]))\n",
    "    w = max([d[1] + d[3] - x for d in digits])\n",
    "    h = max([d[2] + d[4] - y for d in digits])\n",
    "    return x, y, w, h\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's analyse cropped image sizes. This could be relevant to input image size to use in my model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of croped images with size less or equal to 2000 pixels  76.49541943596192\n",
      "Percentage of croped images with size less or equal to 1500 pixels  68.79528171965751\n"
     ]
    }
   ],
   "source": [
    "crop_size = {}\n",
    "for image in os.listdir(root_dir + '/data/train/'):\n",
    "    image_file = os.path.join(root_dir + '/data/train/', image)\n",
    "    if not image.endswith('.png'):\n",
    "        continue\n",
    "    try:\n",
    "        #image_orig = ndimage.imread(image_file, flatten=True)\n",
    "        labels_train_data_dict[image].sort(key=lambda x: x[1])\n",
    "        x, y, w, h = get_rectangle_containing_all_digits(labels_train_data_dict[image])\n",
    "        croped_image_size = int((w*h)/100)\n",
    "        if croped_image_size not in crop_size:\n",
    "            crop_size[croped_image_size] = 0\n",
    "        crop_size[croped_image_size] = crop_size[croped_image_size] + 1\n",
    "    except IOError as e:\n",
    "        print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "        \n",
    "print ('Percentage of croped images with size less or equal to 2000 pixels ', 100.0*sum([v for k,v in crop_size.items() if k <= 20])/len(labels_train_data_dict))\n",
    "print ('Percentage of croped images with size less or equal to 1500 pixels ', 100.0*sum([v for k,v in crop_size.items() if k <= 15])/len(labels_train_data_dict))\n",
    "crop_size = {k:v for k,v in crop_size.items() if k <= 100}\n",
    "plt.bar(list(crop_size.keys()), list(crop_size.values()), align='center')\n",
    "plt.xlim(0, 100)\n",
    "plt.title('Distibution of croped images by size (in 100s of pixels)')\n",
    "plt.ylabel('Number of images')\n",
    "plt.xlabel('Pixels (in 100s pixels)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(83, 167, 3)\n",
      "(48, 9, 67, 71)\n",
      "(71, 67, 3)\n"
     ]
    }
   ],
   "source": [
    "hight = 32\n",
    "width = 64\n",
    "image = misc.imread(root_dir + '/data/train/' + to_display)\n",
    "print (image.shape)\n",
    "print (get_rectangle_containing_all_digits(labels_train_data_dict[to_display]))\n",
    "x, y, w, h = get_rectangle_containing_all_digits(labels_train_data_dict[to_display])\n",
    "image = image[y: y+h, x: x+w]\n",
    "print (image.shape)\n",
    "image = misc.imresize(image, (hight, width))\n",
    "plt.imshow(image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resize pictures to 32*64 grayscale and save them into train.pickle and labels into labels_train.pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def rescale(img, l, height, width):\n",
    "    ph = float(img.shape[0])/height\n",
    "    pw = float(img.shape[1])/width\n",
    "    return [l[0], float(l[1])/ph, float(l[2])/pw, float(l[3])/pw, float(l[4])/ph]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/data/train/train.pickle  saved\n",
      "/data/train/labels_train.pickle  saved\n",
      "/data/test/test.pickle  saved\n",
      "/data/test/labels_test.pickle  saved\n"
     ]
    }
   ],
   "source": [
    "pixel_depth = 255.0\n",
    "hight = 32\n",
    "width = 64\n",
    "\n",
    "def save_to_pickle_file(set_filename, folder, labels, force = False):\n",
    "    if os.path.exists(folder + set_filename) and not force:\n",
    "        print ('Pickle file ', (folder + set_filename), 'already exists.')\n",
    "        return\n",
    "    image_files = os.listdir(root_dir + folder)\n",
    "    d = dict()\n",
    "    dataset = np.ndarray(shape=(len(image_files), hight, width), dtype=np.float32)\n",
    "    num_images = 0\n",
    "    for image in image_files:\n",
    "        if not image.endswith('.png'):\n",
    "            continue\n",
    "        image_file = os.path.join(root_dir + folder, image)\n",
    "        try:\n",
    "            image_orig = ndimage.imread(image_file, flatten=True)\n",
    "            labels[image].sort(key=lambda x: x[1])\n",
    "            d[num_images] = [x[0] for x in labels[image]]\n",
    "            x, y, w, h = get_rectangle_containing_all_digits(labels[image])\n",
    "            #print (image_file , ':', image_orig.shape, ' -> ', x, y, w, h)\n",
    "            image_orig = image_orig[y: y+h, x: x+w]\n",
    "            image_data = (misc.imresize(image_orig, (hight, width)).astype(float) - pixel_depth / 2) / pixel_depth\n",
    "            if image_data.shape != (hight, width):\n",
    "                raise Exception('Unexpected image shape: %s' % str(image_data.shape))\n",
    "            dataset[num_images, :, :] = image_data\n",
    "            num_images = num_images + 1\n",
    "        except IOError as e:\n",
    "            print('Could not read:', image_file, ':', e, '- it\\'s ok, skipping.')\n",
    "    try:\n",
    "        pickle.dump(dataset[0:num_images], open(root_dir + folder + set_filename, 'wb'), pickle.HIGHEST_PROTOCOL)\n",
    "        print ((folder + set_filename), ' saved')\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', set_filename, ':', e)\n",
    "        \n",
    "    try:\n",
    "        file_name = \"labels_\" + set_filename\n",
    "        pickle.dump(d, open( root_dir + folder + file_name, \"wb\" ))\n",
    "        print ((folder + file_name), ' saved')\n",
    "    except Exception as e:\n",
    "        print('Unable to save data to', file_name, ':', e)\n",
    "        \n",
    "\n",
    "save_to_pickle_file('train.pickle', '/data/train/', labels_train_data_dict)\n",
    "save_to_pickle_file('test.pickle',  '/data/test/', labels_test_data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33402, 32, 64)\n",
      "(13068, 32, 64)\n"
     ]
    }
   ],
   "source": [
    "train_data = pickle.load(open(root_dir + '/data/train/train.pickle', 'rb'))\n",
    "print (train_data.shape)\n",
    "labels_train_data = pickle.load(open(root_dir + '/data/train/labels_train.pickle', 'rb'))\n",
    "\n",
    "test_data = pickle.load(open(root_dir + '/data/test/test.pickle', 'rb'))\n",
    "print (test_data.shape)\n",
    "labels_test_data = pickle.load(open(root_dir + '/data/test/labels_test.pickle', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 9, 1]\n"
     ]
    }
   ],
   "source": [
    "# print (train_data[0]*pixel_depth+pixel_depth/2)\n",
    "# print (labels_train_data[0])\n",
    "index = 2\n",
    "fig,ax = plt.subplots(1)\n",
    "plt.imshow(train_data[index], interpolation='nearest')\n",
    "print (labels_train_data[index])\n",
    "#for digit_loc in labels_train_data[index]:\n",
    "#    # Create a Rectangle patch\n",
    "#    rect = patches.Rectangle((digit_loc[1],digit_loc[2]),digit_loc[3],digit_loc[4],linewidth=1,edgecolor='black',facecolor='none')\n",
    "#    # Add the patch to the Axes\n",
    "#    ax.add_patch(rect)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits = 5\n",
    "labels_train = np.zeros((len(labels_train_data), digits, 11))\n",
    "labels_only = {x: [y%10 for y in labels_train_data[x]] for x in labels_train_data}\n",
    "for x in labels_only:\n",
    "    for i in range(digits-len(labels_only[x])):\n",
    "        labels_train[x][i][10] = 1\n",
    "    for i, y in enumerate(labels_only[x]):\n",
    "        labels_train[x][digits-len(labels_only[x])+i][y] = 1 \n",
    "\n",
    "index = 0\n",
    "def get_train_examples(size):\n",
    "    global index\n",
    "    index += size\n",
    "    if index >= train_data.shape[0]:\n",
    "        index = 0\n",
    "        return train_data[index-size: train_data.shape[0]], labels_train[index-size: train_data.shape[0]]\n",
    "    return train_data[index-size:index], labels_train[index-size: index]\n",
    "\n",
    "def get_test_examples(size, start = 0):\n",
    "    labels_ = np.zeros((size+start, digits, 11))\n",
    "    labels_only = {x: [y%10 for y in labels_test_data[x]] for x in labels_test_data}\n",
    "    for x in range(start + size):\n",
    "        for i in range(digits-len(labels_only[x])):\n",
    "            labels_[x][i][10] = 1\n",
    "        for i, y in enumerate(labels_only[x]):\n",
    "            labels_[x][i+digits-len(labels_only[x])][y] = 1\n",
    "    return test_data[start:start+size], labels_[start:start+size]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      " [ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      " [ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "a, b = get_test_examples(1000)\n",
    "print (b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def weight_variable(shape):\n",
    "  initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "  initial = tf.constant(0.1, shape=shape)\n",
    "  return tf.Variable(initial)\n",
    "\n",
    "def conv2d(x, W):\n",
    "  return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "  return tf.nn.max_pool(x, ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import math\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "FLAGS = None\n",
    "\n",
    "pixel_depth = 255.0\n",
    "hight = 32\n",
    "width = 64\n",
    "\n",
    "learning_rate = tf.placeholder(tf.float32, shape=[], name='learning_rate')\n",
    "\n",
    "# Create the model\n",
    "x = tf.placeholder(tf.float32, [None, hight, width], name='input') \n",
    "x_image = tf.reshape(x, [-1, hight, width, 1])\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "## First Convolutional Layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 48])\n",
    "b_conv1 = bias_variable([48])\n",
    "  \n",
    "h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "h_lrn1 = tf.nn.lrn(h_pool1)\n",
    "  \n",
    "## Second Convolutional Layer\n",
    "W_conv2 = weight_variable([5, 5, 48, 128])\n",
    "b_conv2 = bias_variable([128])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_lrn1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "h_lrn2 = tf.nn.lrn(h_pool2)\n",
    "\n",
    "## Third Convolutional Layer\n",
    "W_conv3 = weight_variable([5, 5, 128, 192])\n",
    "b_conv3 = bias_variable([192])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_lrn2, W_conv3) + b_conv3)\n",
    "h_lrn3 = tf.nn.lrn(h_conv3)\n",
    "\n",
    "## Forth Convolutional Layer\n",
    "W_conv4 = weight_variable([3, 3, 192, 192])\n",
    "b_conv4 = bias_variable([192])\n",
    "\n",
    "h_conv4 = tf.nn.relu(conv2d(h_lrn3, W_conv4) + b_conv4)\n",
    "h_lrn4 = tf.nn.lrn(h_conv4)\n",
    "\n",
    "## Fifth Convolutional Layer\n",
    "W_conv5 = weight_variable([3, 3, 192, 192])\n",
    "b_conv5 = bias_variable([192])\n",
    "\n",
    "h_conv5 = tf.nn.relu(conv2d(h_lrn4, W_conv5) + b_conv5)\n",
    "h_lrn5 = tf.nn.lrn(h_conv5)\n",
    "\n",
    "## Densely Connected Layer 1\n",
    "W_fc1 = weight_variable([hight * width * 12, 2048])\n",
    "b_fc1 = bias_variable([2048])\n",
    "\n",
    "h_pool2_flat = tf.reshape(h_lrn5, [-1, hight * width * 12])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "## Densely Connected Layer 2\n",
    "W_fc2 = weight_variable([2048, 2048])\n",
    "b_fc2 = bias_variable([2048])\n",
    "\n",
    "h_fc2 = tf.nn.relu(tf.matmul(h_fc1, W_fc2) + b_fc2)\n",
    "h_fc2_drop = tf.nn.dropout(h_fc2, keep_prob)\n",
    "\n",
    "## Readout Layer\n",
    "W_fc3 = weight_variable([2048, 11*digits])\n",
    "b_fc3 = bias_variable([11*digits])\n",
    "\n",
    "y_conv = tf.matmul(h_fc2_drop, W_fc3) + b_fc3\n",
    "y_conv_r = tf.reshape(y_conv, [-1, digits, 11])\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, digits, 11], name='output')\n",
    "#y_r = tf.reshape(y, [-1, digits, 11])\n",
    "\n",
    "cross_entropy = tf.reduce_mean(  math.pow(1.2, 0)*tf.nn.softmax_cross_entropy_with_logits(y_conv_r[0], y[0]) \n",
    "                               + math.pow(1.2, 1)*tf.nn.softmax_cross_entropy_with_logits(y_conv_r[1], y[1])\n",
    "                               + math.pow(1.2, 2)*tf.nn.softmax_cross_entropy_with_logits(y_conv_r[2], y[2])\n",
    "                               + math.pow(1.2, 3)*tf.nn.softmax_cross_entropy_with_logits(y_conv_r[3], y[3])\n",
    "                               + math.pow(1.2, 4)*tf.nn.softmax_cross_entropy_with_logits(y_conv_r[4], y[4]))\n",
    "\n",
    "train_step = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cross_entropy)\n",
    "\n",
    "accuracy_list = list()\n",
    "loss_list = list()\n",
    "\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv_r, 2), tf.argmax(y, 2))\n",
    "accuracy = tf.reduce_mean(tf.reduce_min(tf.cast(correct_prediction, tf.float32), 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.004, loss 128.369\n",
      "step 100, training accuracy 0.011, loss 35.2186\n",
      "step 200, training accuracy 0.016, loss 33.9467\n",
      "step 300, training accuracy 0.026, loss 20.1758\n",
      "step 400, training accuracy 0.026, loss 19.7637\n",
      "step 500, training accuracy 0.054, loss 17.697\n",
      "step 600, training accuracy 0.0435, loss 10.7357\n",
      "step 700, training accuracy 0.015, loss 16.2858\n",
      "step 800, training accuracy 0.0615, loss 15.3847\n",
      "step 900, training accuracy 0.04, loss 13.2707\n",
      "step 1000, training accuracy 0.0495, loss 14.1013\n",
      "step 1100, training accuracy 0.0675, loss 15.1175\n",
      "step 1200, training accuracy 0.0925, loss 12.7913\n",
      "step 1300, training accuracy 0.0775, loss 9.37714\n",
      "step 1400, training accuracy 0.073, loss 18.558\n",
      "step 1500, training accuracy 0.101, loss 11.406\n",
      "step 1600, training accuracy 0.11, loss 14.9212\n",
      "step 1700, training accuracy 0.092, loss 19.5091\n",
      "step 1800, training accuracy 0.083, loss 17.9322\n",
      "step 1900, training accuracy 0.1205, loss 13.0193\n",
      "step 2000, training accuracy 0.1135, loss 16.2399\n",
      "step 2100, training accuracy 0.117, loss 16.9828\n",
      "step 2200, training accuracy 0.0665, loss 14.0212\n",
      "step 2300, training accuracy 0.146, loss 14.7713\n",
      "step 2400, training accuracy 0.117, loss 11.0716\n",
      "step 2500, training accuracy 0.15, loss 10.8203\n",
      "step 2600, training accuracy 0.1155, loss 17.7105\n",
      "step 2700, training accuracy 0.137, loss 13.8669\n",
      "step 2800, training accuracy 0.173, loss 15.67\n",
      "step 2900, training accuracy 0.177, loss 14.6525\n",
      "step 3000, training accuracy 0.1555, loss 11.8925\n",
      "step 3100, training accuracy 0.211, loss 16.5002\n",
      "step 3200, training accuracy 0.2335, loss 11.6676\n",
      "step 3300, training accuracy 0.196, loss 13.0672\n",
      "step 3400, training accuracy 0.1775, loss 12.7937\n",
      "step 3500, training accuracy 0.24, loss 9.41241\n",
      "step 3600, training accuracy 0.2585, loss 11.7887\n",
      "step 3700, training accuracy 0.214, loss 12.2676\n",
      "step 3800, training accuracy 0.258, loss 12.5809\n",
      "step 3900, training accuracy 0.227, loss 13.9922\n",
      "step 4000, training accuracy 0.263, loss 13.0455\n",
      "step 4100, training accuracy 0.258, loss 12.7739\n",
      "step 4200, training accuracy 0.267, loss 10.9433\n",
      "step 4300, training accuracy 0.272, loss 10.9875\n",
      "step 4400, training accuracy 0.247, loss 10.5066\n",
      "step 4500, training accuracy 0.309, loss 8.16718\n",
      "step 4600, training accuracy 0.2535, loss 12.9798\n",
      "step 4700, training accuracy 0.271, loss 9.70259\n",
      "step 4800, training accuracy 0.291, loss 12.048\n",
      "step 4900, training accuracy 0.264, loss 14.5691\n",
      "step 5000, training accuracy 0.2745, loss 11.8642\n",
      "step 5100, training accuracy 0.2785, loss 10.126\n",
      "step 5200, training accuracy 0.2835, loss 9.04411\n",
      "step 5300, training accuracy 0.309, loss 11.1368\n",
      "step 5400, training accuracy 0.3045, loss 8.54208\n",
      "step 5500, training accuracy 0.299, loss 7.87198\n",
      "step 5600, training accuracy 0.27, loss 13.0181\n",
      "step 5700, training accuracy 0.2555, loss 15.2846\n",
      "step 5800, training accuracy 0.2925, loss 11.8495\n",
      "step 5900, training accuracy 0.2895, loss 11.5061\n",
      "step 6000, training accuracy 0.36, loss 13.4174\n",
      "step 6100, training accuracy 0.322, loss 14.0621\n",
      "step 6200, training accuracy 0.3525, loss 12.6713\n",
      "step 6300, training accuracy 0.3125, loss 12.7732\n",
      "step 6400, training accuracy 0.3375, loss 17.388\n",
      "step 6500, training accuracy 0.3605, loss 10.3695\n",
      "step 6600, training accuracy 0.342, loss 13.7498\n",
      "step 6700, training accuracy 0.337, loss 15.3501\n",
      "step 6800, training accuracy 0.359, loss 16.3632\n",
      "step 6900, training accuracy 0.3535, loss 16.5331\n",
      "step 7000, training accuracy 0.353, loss 14.7676\n",
      "step 7100, training accuracy 0.3765, loss 16.0802\n",
      "step 7200, training accuracy 0.3865, loss 15.3811\n",
      "step 7300, training accuracy 0.352, loss 17.8542\n",
      "step 7400, training accuracy 0.354, loss 12.0641\n",
      "step 7500, training accuracy 0.3875, loss 15.3051\n",
      "step 7600, training accuracy 0.3395, loss 12.5276\n",
      "step 7700, training accuracy 0.381, loss 14.7263\n",
      "step 7800, training accuracy 0.347, loss 12.5134\n",
      "step 7900, training accuracy 0.347, loss 12.1823\n",
      "step 8000, training accuracy 0.375, loss 13.832\n",
      "step 8100, training accuracy 0.3255, loss 11.6357\n",
      "step 8200, training accuracy 0.3495, loss 15.1489\n",
      "step 8300, training accuracy 0.369, loss 15.3975\n",
      "step 8400, training accuracy 0.3805, loss 14.7897\n",
      "step 8500, training accuracy 0.3765, loss 18.6876\n",
      "step 8600, training accuracy 0.3715, loss 20.2601\n",
      "step 8700, training accuracy 0.37, loss 15.2788\n",
      "step 8800, training accuracy 0.3825, loss 14.1098\n",
      "step 8900, training accuracy 0.315, loss 17.1766\n",
      "step 9000, training accuracy 0.378, loss 12.7671\n",
      "step 9100, training accuracy 0.313, loss 16.0515\n",
      "step 9200, training accuracy 0.3825, loss 18.6807\n",
      "step 9300, training accuracy 0.3725, loss 18.2756\n",
      "step 9400, training accuracy 0.382, loss 18.888\n",
      "step 9500, training accuracy 0.3625, loss 17.36\n",
      "step 9600, training accuracy 0.356, loss 20.8712\n",
      "step 9700, training accuracy 0.388, loss 18.248\n",
      "step 9800, training accuracy 0.397, loss 16.3549\n",
      "step 9900, training accuracy 0.385, loss 19.3163\n",
      "step 10000, training accuracy 0.403, loss 17.4086\n",
      "step 10100, training accuracy 0.3885, loss 16.9497\n",
      "step 10200, training accuracy 0.3975, loss 16.076\n",
      "step 10300, training accuracy 0.4105, loss 14.1182\n",
      "step 10400, training accuracy 0.414, loss 14.3703\n",
      "step 10500, training accuracy 0.4025, loss 18.9182\n",
      "step 10600, training accuracy 0.389, loss 20.6284\n",
      "step 10700, training accuracy 0.408, loss 22.394\n",
      "step 10800, training accuracy 0.393, loss 19.7188\n",
      "step 10900, training accuracy 0.328, loss 15.281\n",
      "step 11000, training accuracy 0.3865, loss 19.6067\n",
      "step 11100, training accuracy 0.419, loss 17.0815\n",
      "step 11200, training accuracy 0.412, loss 14.4608\n",
      "step 11300, training accuracy 0.395, loss 13.62\n",
      "step 11400, training accuracy 0.419, loss 20.3767\n",
      "step 11500, training accuracy 0.377, loss 19.2602\n",
      "step 11600, training accuracy 0.403, loss 14.9612\n",
      "step 11700, training accuracy 0.3765, loss 17.4623\n",
      "step 11800, training accuracy 0.397, loss 20.397\n",
      "step 11900, training accuracy 0.4215, loss 14.8303\n",
      "step 12000, training accuracy 0.387, loss 19.0504\n",
      "step 12100, training accuracy 0.429, loss 12.6798\n",
      "step 12200, training accuracy 0.435, loss 12.9768\n",
      "step 12300, training accuracy 0.432, loss 13.8115\n",
      "step 12400, training accuracy 0.442, loss 17.6744\n",
      "step 12500, training accuracy 0.406, loss 14.4882\n",
      "step 12600, training accuracy 0.4265, loss 14.4557\n",
      "step 12700, training accuracy 0.453, loss 14.4355\n",
      "step 12800, training accuracy 0.351, loss 16.4362\n",
      "step 12900, training accuracy 0.424, loss 17.5914\n",
      "step 13000, training accuracy 0.413, loss 20.2133\n",
      "step 13100, training accuracy 0.4, loss 29.8884\n",
      "step 13200, training accuracy 0.4465, loss 19.7499\n",
      "step 13300, training accuracy 0.41, loss 16.7797\n",
      "step 13400, training accuracy 0.418, loss 25.3304\n",
      "step 13500, training accuracy 0.433, loss 21.9885\n",
      "step 13600, training accuracy 0.44, loss 15.4275\n",
      "step 13700, training accuracy 0.439, loss 11.9382\n",
      "step 13800, training accuracy 0.3755, loss 14.4513\n",
      "step 13900, training accuracy 0.411, loss 13.8304\n",
      "step 14000, training accuracy 0.4345, loss 15.1114\n",
      "step 14100, training accuracy 0.429, loss 15.8392\n",
      "step 14200, training accuracy 0.3965, loss 16.7451\n",
      "step 14300, training accuracy 0.413, loss 18.9131\n",
      "step 14400, training accuracy 0.4525, loss 29.1833\n",
      "step 14500, training accuracy 0.423, loss 22.0706\n",
      "step 14600, training accuracy 0.408, loss 19.8448\n",
      "step 14700, training accuracy 0.3935, loss 16.5297\n",
      "step 14800, training accuracy 0.415, loss 17.6455\n",
      "step 14900, training accuracy 0.441, loss 14.5097\n",
      "step 15000, training accuracy 0.4485, loss 17.0701\n",
      "step 15100, training accuracy 0.3985, loss 24.3249\n",
      "step 15200, training accuracy 0.4425, loss 15.3359\n",
      "step 15300, training accuracy 0.43, loss 17.4035\n",
      "step 15400, training accuracy 0.4325, loss 16.423\n",
      "step 15500, training accuracy 0.4315, loss 12.401\n",
      "step 15600, training accuracy 0.465, loss 17.5717\n",
      "step 15700, training accuracy 0.445, loss 17.3331\n",
      "step 15800, training accuracy 0.354, loss 19.269\n",
      "step 15900, training accuracy 0.4275, loss 15.375\n",
      "step 16000, training accuracy 0.425, loss 15.6188\n",
      "step 16100, training accuracy 0.4435, loss 15.1507\n",
      "step 16200, training accuracy 0.4465, loss 13.1236\n",
      "step 16300, training accuracy 0.436, loss 12.897\n",
      "step 16400, training accuracy 0.4385, loss 14.6571\n",
      "step 16500, training accuracy 0.4085, loss 24.0268\n",
      "step 16600, training accuracy 0.4315, loss 19.394\n",
      "step 16700, training accuracy 0.4565, loss 20.7256\n",
      "step 16800, training accuracy 0.423, loss 24.9041\n",
      "step 16900, training accuracy 0.448, loss 16.34\n",
      "step 17000, training accuracy 0.439, loss 19.4592\n",
      "step 17100, training accuracy 0.453, loss 26.1425\n",
      "step 17200, training accuracy 0.4405, loss 22.9785\n",
      "step 17300, training accuracy 0.437, loss 21.3697\n",
      "step 17400, training accuracy 0.463, loss 17.882\n",
      "step 17500, training accuracy 0.454, loss 16.7448\n",
      "step 17600, training accuracy 0.459, loss 14.8484\n",
      "step 17700, training accuracy 0.4425, loss 21.0749\n",
      "step 17800, training accuracy 0.468, loss 22.3145\n",
      "step 17900, training accuracy 0.441, loss 25.1683\n",
      "step 18000, training accuracy 0.4325, loss 18.6042\n",
      "step 18100, training accuracy 0.4125, loss 26.2616\n",
      "step 18200, training accuracy 0.4425, loss 22.9179\n",
      "step 18300, training accuracy 0.4535, loss 18.6074\n",
      "step 18400, training accuracy 0.447, loss 19.4083\n",
      "step 18500, training accuracy 0.4325, loss 21.4592\n",
      "step 18900, training accuracy 0.4505, loss 23.1344\n",
      "step 19000, training accuracy 0.465, loss 21.0333\n",
      "step 19100, training accuracy 0.436, loss 16.8323\n",
      "step 19200, training accuracy 0.464, loss 14.9342\n",
      "step 19300, training accuracy 0.465, loss 19.4742\n",
      "step 19400, training accuracy 0.465, loss 16.7202\n",
      "step 19500, training accuracy 0.4305, loss 14.032\n",
      "step 19600, training accuracy 0.4625, loss 21.1327\n",
      "step 19700, training accuracy 0.449, loss 18.7726\n",
      "step 19800, training accuracy 0.4265, loss 24.1495\n",
      "step 19900, training accuracy 0.458, loss 20.6782\n",
      "step 20000, training accuracy 0.4255, loss 23.1108\n",
      "step 20100, training accuracy 0.453, loss 19.9337\n",
      "step 20200, training accuracy 0.4445, loss 23.9121\n",
      "step 20300, training accuracy 0.43, loss 16.5549\n",
      "step 20400, training accuracy 0.4245, loss 15.7604\n",
      "step 20500, training accuracy 0.436, loss 16.3313\n",
      "step 20600, training accuracy 0.4845, loss 16.0938\n",
      "step 20700, training accuracy 0.4495, loss 17.092\n",
      "step 20800, training accuracy 0.451, loss 18.8929\n",
      "step 20900, training accuracy 0.4525, loss 12.4872\n",
      "step 21000, training accuracy 0.473, loss 19.6708\n",
      "step 21100, training accuracy 0.455, loss 12.4033\n",
      "step 21200, training accuracy 0.465, loss 19.8346\n",
      "step 21300, training accuracy 0.4495, loss 19.7279\n",
      "step 21400, training accuracy 0.4525, loss 22.2107\n",
      "step 21500, training accuracy 0.444, loss 15.771\n",
      "step 21600, training accuracy 0.47, loss 14.2458\n",
      "step 21700, training accuracy 0.4655, loss 30.4357\n",
      "step 21800, training accuracy 0.456, loss 14.6047\n",
      "step 21900, training accuracy 0.4655, loss 17.2731\n",
      "step 22000, training accuracy 0.4775, loss 17.8388\n",
      "step 22100, training accuracy 0.4605, loss 18.6359\n",
      "step 22200, training accuracy 0.4465, loss 24.9288\n",
      "step 22300, training accuracy 0.4415, loss 16.3027\n",
      "step 22400, training accuracy 0.432, loss 18.3736\n",
      "step 22500, training accuracy 0.454, loss 16.1133\n",
      "step 22600, training accuracy 0.454, loss 17.2204\n",
      "step 22700, training accuracy 0.457, loss 11.6294\n",
      "step 22800, training accuracy 0.449, loss 12.1651\n",
      "step 22900, training accuracy 0.4375, loss 12.4878\n",
      "step 23000, training accuracy 0.457, loss 13.9496\n",
      "step 23100, training accuracy 0.432, loss 20.2587\n",
      "step 23200, training accuracy 0.4425, loss 23.381\n",
      "step 23300, training accuracy 0.449, loss 17.7026\n",
      "step 23400, training accuracy 0.4635, loss 23.0195\n",
      "step 23500, training accuracy 0.4405, loss 22.6343\n",
      "step 23600, training accuracy 0.439, loss 20.4951\n",
      "step 23700, training accuracy 0.441, loss 21.7932\n",
      "step 23800, training accuracy 0.452, loss 25.7046\n",
      "step 23900, training accuracy 0.4305, loss 19.3662\n",
      "step 24000, training accuracy 0.466, loss 17.3499\n",
      "step 24100, training accuracy 0.458, loss 23.0772\n",
      "step 24200, training accuracy 0.4815, loss 24.1724\n",
      "step 24300, training accuracy 0.4655, loss 24.2179\n",
      "step 24400, training accuracy 0.447, loss 23.661\n",
      "step 24500, training accuracy 0.455, loss 24.6878\n",
      "step 24600, training accuracy 0.4755, loss 19.9465\n",
      "step 24700, training accuracy 0.487, loss 15.1409\n",
      "step 24800, training accuracy 0.477, loss 18.0389\n",
      "step 24900, training accuracy 0.492, loss 22.2989\n",
      "step 25000, training accuracy 0.4585, loss 22.3\n",
      "step 25100, training accuracy 0.468, loss 27.9657\n",
      "step 25200, training accuracy 0.436, loss 24.4354\n",
      "step 25300, training accuracy 0.4785, loss 24.26\n",
      "step 25400, training accuracy 0.4735, loss 24.992\n",
      "step 25500, training accuracy 0.4995, loss 28.4526\n",
      "step 25600, training accuracy 0.49, loss 29.4487\n",
      "step 25700, training accuracy 0.4705, loss 23.8812\n",
      "step 25800, training accuracy 0.4585, loss 27.4084\n",
      "step 25900, training accuracy 0.4595, loss 29.6892\n",
      "step 26000, training accuracy 0.473, loss 20.8763\n",
      "step 26100, training accuracy 0.4675, loss 19.4719\n",
      "step 26200, training accuracy 0.469, loss 19.6608\n",
      "step 26300, training accuracy 0.4595, loss 21.7495\n",
      "step 26400, training accuracy 0.46, loss 19.6765\n",
      "step 26500, training accuracy 0.475, loss 20.1156\n",
      "step 26600, training accuracy 0.4765, loss 23.6252\n",
      "step 26700, training accuracy 0.475, loss 21.8444\n",
      "step 26800, training accuracy 0.4705, loss 19.0759\n",
      "step 26900, training accuracy 0.4695, loss 24.859\n",
      "step 27000, training accuracy 0.4625, loss 27.9622\n",
      "step 27100, training accuracy 0.4835, loss 17.8602\n",
      "step 27200, training accuracy 0.448, loss 17.8045\n",
      "step 27300, training accuracy 0.481, loss 15.7681\n",
      "step 27400, training accuracy 0.449, loss 18.1295\n",
      "step 27500, training accuracy 0.456, loss 19.3653\n",
      "step 27600, training accuracy 0.445, loss 15.7602\n",
      "step 27700, training accuracy 0.475, loss 29.3995\n",
      "step 27800, training accuracy 0.471, loss 25.1873\n",
      "step 27900, training accuracy 0.478, loss 21.9181\n",
      "step 28000, training accuracy 0.476, loss 21.8529\n",
      "step 28100, training accuracy 0.493, loss 29.72\n",
      "step 28200, training accuracy 0.4895, loss 20.0074\n",
      "step 28300, training accuracy 0.484, loss 24.0467\n",
      "step 28400, training accuracy 0.4985, loss 26.7591\n",
      "step 28500, training accuracy 0.505, loss 24.829\n",
      "step 28600, training accuracy 0.4995, loss 22.886\n",
      "step 28700, training accuracy 0.4915, loss 17.7554\n",
      "step 28800, training accuracy 0.483, loss 24.5104\n",
      "step 28900, training accuracy 0.48, loss 25.3068\n",
      "step 29000, training accuracy 0.4745, loss 16.2518\n",
      "step 29100, training accuracy 0.5035, loss 20.0247\n",
      "step 29200, training accuracy 0.4905, loss 18.243\n",
      "step 29300, training accuracy 0.4445, loss 22.881\n",
      "step 29400, training accuracy 0.503, loss 16.3075\n",
      "step 29500, training accuracy 0.5045, loss 20.2843\n",
      "step 29600, training accuracy 0.502, loss 21.745\n",
      "step 29700, training accuracy 0.4595, loss 15.2205\n",
      "step 29800, training accuracy 0.4745, loss 14.5187\n",
      "step 29900, training accuracy 0.457, loss 21.9874\n",
      "step 30000, training accuracy 0.469, loss 15.2212\n",
      "step 30100, training accuracy 0.4815, loss 16.6404\n",
      "step 30200, training accuracy 0.4725, loss 19.8857\n",
      "step 30300, training accuracy 0.504, loss 14.2961\n",
      "step 30400, training accuracy 0.4995, loss 19.5376\n",
      "step 30500, training accuracy 0.488, loss 16.6818\n",
      "step 30600, training accuracy 0.496, loss 16.4845\n",
      "step 30700, training accuracy 0.4805, loss 19.9379\n",
      "step 30800, training accuracy 0.4655, loss 26.4589\n",
      "step 30900, training accuracy 0.4625, loss 24.1737\n",
      "step 31000, training accuracy 0.4725, loss 20.617\n",
      "step 31100, training accuracy 0.456, loss 17.0007\n",
      "step 31200, training accuracy 0.4855, loss 16.9359\n",
      "step 31300, training accuracy 0.4835, loss 21.0234\n",
      "step 31400, training accuracy 0.4785, loss 19.7746\n",
      "step 31500, training accuracy 0.4865, loss 16.9156\n",
      "step 31600, training accuracy 0.485, loss 16.1189\n",
      "step 31700, training accuracy 0.4815, loss 24.7993\n",
      "step 31800, training accuracy 0.488, loss 24.7958\n",
      "step 31900, training accuracy 0.468, loss 20.0098\n",
      "step 32000, training accuracy 0.492, loss 26.2875\n",
      "step 32100, training accuracy 0.5005, loss 20.8362\n",
      "step 32200, training accuracy 0.499, loss 22.111\n",
      "step 32300, training accuracy 0.506, loss 23.9412\n",
      "step 32400, training accuracy 0.4955, loss 20.9193\n",
      "step 32500, training accuracy 0.5115, loss 27.6419\n",
      "step 32600, training accuracy 0.498, loss 23.8233\n",
      "step 32700, training accuracy 0.4965, loss 24.5074\n",
      "step 32800, training accuracy 0.4925, loss 23.8195\n",
      "step 32900, training accuracy 0.4945, loss 23.6806\n",
      "step 33000, training accuracy 0.483, loss 23.6276\n",
      "step 33100, training accuracy 0.481, loss 24.3066\n",
      "step 33200, training accuracy 0.4845, loss 27.1058\n",
      "step 33300, training accuracy 0.4845, loss 21.4911\n",
      "step 33400, training accuracy 0.499, loss 26.6915\n",
      "step 33500, training accuracy 0.492, loss 23.0669\n",
      "step 33600, training accuracy 0.453, loss 25.3073\n",
      "step 33700, training accuracy 0.4535, loss 25.6847\n",
      "step 33800, training accuracy 0.461, loss 28.5427\n",
      "step 33900, training accuracy 0.475, loss 25.9946\n",
      "step 34000, training accuracy 0.468, loss 33.8665\n",
      "step 34100, training accuracy 0.4895, loss 22.2174\n",
      "step 34200, training accuracy 0.475, loss 29.8097\n",
      "step 34300, training accuracy 0.465, loss 27.143\n",
      "step 34400, training accuracy 0.4735, loss 21.883\n",
      "step 34500, training accuracy 0.5075, loss 26.0563\n",
      "step 34600, training accuracy 0.4695, loss 26.1172\n",
      "step 34700, training accuracy 0.463, loss 31.6308\n",
      "step 34800, training accuracy 0.4735, loss 29.635\n",
      "step 34900, training accuracy 0.4885, loss 19.8742\n",
      "step 35000, training accuracy 0.4795, loss 24.4021\n",
      "step 35100, training accuracy 0.49, loss 16.3159\n",
      "step 35200, training accuracy 0.495, loss 20.8759\n",
      "step 35300, training accuracy 0.486, loss 15.7382\n",
      "step 35400, training accuracy 0.478, loss 21.2321\n",
      "step 35500, training accuracy 0.4725, loss 19.8615\n",
      "step 35600, training accuracy 0.4925, loss 19.8223\n",
      "step 35700, training accuracy 0.4785, loss 27.9914\n",
      "step 35800, training accuracy 0.48, loss 25.8707\n",
      "step 35900, training accuracy 0.5025, loss 25.3828\n",
      "step 36000, training accuracy 0.4965, loss 21.1651\n",
      "step 36100, training accuracy 0.456, loss 22.4787\n",
      "step 36200, training accuracy 0.5055, loss 19.9994\n",
      "step 36300, training accuracy 0.499, loss 23.9946\n",
      "step 36400, training accuracy 0.4765, loss 22.3227\n",
      "step 36500, training accuracy 0.4665, loss 32.1563\n",
      "step 36600, training accuracy 0.4675, loss 26.4974\n",
      "step 36700, training accuracy 0.49, loss 20.6684\n",
      "step 36800, training accuracy 0.508, loss 23.4431\n",
      "step 36900, training accuracy 0.5, loss 22.9284\n",
      "step 37000, training accuracy 0.5015, loss 22.7835\n",
      "step 37100, training accuracy 0.5015, loss 24.1316\n",
      "step 37200, training accuracy 0.4615, loss 25.4248\n",
      "step 37300, training accuracy 0.486, loss 23.0407\n",
      "step 37400, training accuracy 0.5235, loss 21.2568\n",
      "step 37500, training accuracy 0.4965, loss 23.8516\n",
      "step 37600, training accuracy 0.506, loss 24.4369\n",
      "step 37700, training accuracy 0.4865, loss 22.5136\n",
      "step 37800, training accuracy 0.497, loss 24.4336\n",
      "step 37900, training accuracy 0.4805, loss 25.409\n",
      "step 38000, training accuracy 0.505, loss 26.3808\n",
      "step 38100, training accuracy 0.494, loss 23.6331\n",
      "step 38200, training accuracy 0.493, loss 34.062\n",
      "step 38300, training accuracy 0.4945, loss 27.8173\n",
      "step 38400, training accuracy 0.495, loss 23.1452\n",
      "step 38500, training accuracy 0.4505, loss 24.3671\n",
      "step 38600, training accuracy 0.4835, loss 21.78\n",
      "step 38700, training accuracy 0.4925, loss 19.0102\n",
      "step 38800, training accuracy 0.4925, loss 23.3098\n",
      "step 38900, training accuracy 0.4955, loss 24.1908\n",
      "step 39000, training accuracy 0.49, loss 25.9727\n",
      "step 39100, training accuracy 0.504, loss 25.9804\n",
      "step 39200, training accuracy 0.494, loss 26.6844\n",
      "step 39300, training accuracy 0.5105, loss 23.9981\n",
      "step 39400, training accuracy 0.457, loss 25.7944\n",
      "step 39500, training accuracy 0.497, loss 29.1607\n",
      "step 39600, training accuracy 0.474, loss 31.3419\n",
      "step 39700, training accuracy 0.501, loss 29.1528\n",
      "step 39800, training accuracy 0.4855, loss 27.541\n",
      "step 39900, training accuracy 0.486, loss 23.5895\n",
      "step 40000, training accuracy 0.4885, loss 22.5505\n",
      "step 40100, training accuracy 0.4915, loss 23.1404\n",
      "step 40200, training accuracy 0.507, loss 30.2593\n",
      "step 40300, training accuracy 0.5055, loss 33.3406\n",
      "step 40400, training accuracy 0.505, loss 35.1603\n",
      "step 40500, training accuracy 0.5135, loss 26.412\n",
      "step 40600, training accuracy 0.5125, loss 29.7944\n",
      "step 40700, training accuracy 0.495, loss 23.0926\n",
      "step 40800, training accuracy 0.503, loss 23.3425\n",
      "step 40900, training accuracy 0.5025, loss 27.2569\n",
      "step 41000, training accuracy 0.49, loss 22.0796\n",
      "step 41100, training accuracy 0.502, loss 29.5516\n",
      "step 41200, training accuracy 0.523, loss 25.4558\n",
      "step 41300, training accuracy 0.5085, loss 26.3813\n",
      "step 41400, training accuracy 0.4715, loss 23.0319\n",
      "step 41500, training accuracy 0.5075, loss 25.4612\n",
      "step 41600, training accuracy 0.513, loss 27.99\n",
      "step 41700, training accuracy 0.4835, loss 27.9615\n",
      "step 41800, training accuracy 0.5125, loss 23.7423\n",
      "step 41900, training accuracy 0.4775, loss 30.4425\n",
      "step 42000, training accuracy 0.4835, loss 17.7327\n",
      "step 42100, training accuracy 0.5025, loss 15.9498\n",
      "step 42200, training accuracy 0.508, loss 15.511\n",
      "step 42300, training accuracy 0.498, loss 18.9458\n",
      "step 42400, training accuracy 0.4695, loss 19.0853\n",
      "step 42500, training accuracy 0.495, loss 26.0291\n",
      "step 42600, training accuracy 0.468, loss 22.5048\n",
      "step 42700, training accuracy 0.4885, loss 23.4421\n",
      "step 42800, training accuracy 0.49, loss 15.6558\n",
      "step 42900, training accuracy 0.481, loss 13.8583\n",
      "step 43000, training accuracy 0.5045, loss 17.8606\n",
      "step 43100, training accuracy 0.484, loss 15.875\n",
      "step 43200, training accuracy 0.4945, loss 14.9699\n",
      "step 43300, training accuracy 0.5255, loss 12.2432\n",
      "step 43400, training accuracy 0.5195, loss 15.0022\n",
      "step 43500, training accuracy 0.4995, loss 20.7106\n",
      "step 43600, training accuracy 0.488, loss 18.1706\n",
      "step 43700, training accuracy 0.514, loss 14.3517\n",
      "step 43800, training accuracy 0.509, loss 14.2114\n",
      "step 43900, training accuracy 0.511, loss 12.4425\n",
      "step 44000, training accuracy 0.4785, loss 14.8389\n",
      "step 44100, training accuracy 0.5025, loss 18.7074\n",
      "step 44200, training accuracy 0.5155, loss 21.6502\n",
      "step 44300, training accuracy 0.514, loss 16.0509\n",
      "step 44400, training accuracy 0.5105, loss 11.2667\n",
      "step 44500, training accuracy 0.515, loss 7.66962\n",
      "step 44600, training accuracy 0.4785, loss 13.1088\n",
      "step 44700, training accuracy 0.4985, loss 15.6528\n",
      "step 44800, training accuracy 0.507, loss 18.5915\n",
      "step 44900, training accuracy 0.52, loss 15.5251\n",
      "step 45000, training accuracy 0.5155, loss 17.6642\n",
      "step 45100, training accuracy 0.4975, loss 18.0798\n",
      "step 45200, training accuracy 0.4415, loss 21.9209\n",
      "step 45300, training accuracy 0.4775, loss 23.6205\n",
      "step 45400, training accuracy 0.5035, loss 15.7601\n",
      "step 45500, training accuracy 0.4905, loss 19.7844\n",
      "step 45600, training accuracy 0.5275, loss 19.8062\n",
      "step 45700, training accuracy 0.515, loss 18.7053\n",
      "step 45800, training accuracy 0.512, loss 27.437\n",
      "step 45900, training accuracy 0.533, loss 26.4657\n",
      "step 46000, training accuracy 0.5225, loss 26.5824\n",
      "step 46100, training accuracy 0.5115, loss 29.37\n",
      "step 46200, training accuracy 0.5285, loss 27.4468\n",
      "step 46300, training accuracy 0.53, loss 31.3096\n",
      "step 46400, training accuracy 0.5245, loss 30.2912\n",
      "step 46500, training accuracy 0.5355, loss 31.7897\n",
      "step 46600, training accuracy 0.5295, loss 28.1705\n",
      "step 46700, training accuracy 0.4985, loss 22.0446\n",
      "step 46800, training accuracy 0.5105, loss 20.9344\n",
      "step 46900, training accuracy 0.5225, loss 18.4802\n",
      "step 47000, training accuracy 0.512, loss 18.6395\n",
      "step 47100, training accuracy 0.5, loss 14.1037\n",
      "step 47200, training accuracy 0.5175, loss 17.0087\n",
      "step 47300, training accuracy 0.496, loss 24.1686\n",
      "step 47400, training accuracy 0.4855, loss 19.5567\n",
      "step 47500, training accuracy 0.462, loss 14.3004\n",
      "step 47600, training accuracy 0.5015, loss 17.8847\n",
      "step 47700, training accuracy 0.4855, loss 17.3958\n",
      "step 47800, training accuracy 0.5125, loss 14.2415\n",
      "step 47900, training accuracy 0.515, loss 21.8066\n",
      "step 48000, training accuracy 0.518, loss 21.6132\n",
      "step 48100, training accuracy 0.524, loss 15.8383\n",
      "step 48200, training accuracy 0.5035, loss 22.8101\n",
      "step 48300, training accuracy 0.5185, loss 16.634\n",
      "step 48400, training accuracy 0.5125, loss 22.095\n",
      "step 48500, training accuracy 0.5005, loss 29.4042\n",
      "step 48600, training accuracy 0.498, loss 23.5461\n",
      "step 48700, training accuracy 0.4945, loss 17.3138\n",
      "step 48800, training accuracy 0.509, loss 17.9409\n",
      "step 48900, training accuracy 0.494, loss 24.2829\n",
      "step 49000, training accuracy 0.493, loss 16.5443\n",
      "step 49100, training accuracy 0.474, loss 24.0037\n",
      "step 49200, training accuracy 0.4805, loss 15.2392\n",
      "step 49300, training accuracy 0.48, loss 25.4262\n",
      "step 49400, training accuracy 0.5125, loss 22.7546\n",
      "step 49500, training accuracy 0.4835, loss 29.1159\n",
      "step 49600, training accuracy 0.498, loss 24.6582\n",
      "step 49700, training accuracy 0.518, loss 24.3811\n",
      "step 49800, training accuracy 0.52, loss 29.6115\n",
      "step 49900, training accuracy 0.523, loss 27.4835\n",
      "step 50000, training accuracy 0.535, loss 30.708\n",
      "test accuracy 0.535\n",
      "Model saved in file: /home/ubuntu/pynb/SVHN/tmp/model.ckpt\n"
     ]
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "sess.run(tf.initialize_all_variables())\n",
    "for i in range(50001):\n",
    "    batch_images, batch_labels = get_train_examples(50)\n",
    "    lr = 1.0e-4\n",
    "    loss = train_step.run(feed_dict={x: batch_images, y: batch_labels, keep_prob: 1.0, learning_rate: lr})\n",
    "    if i%100 == 0:\n",
    "        batch_images, batch_labels = get_test_examples(2000)\n",
    "        loss, train_accuracy = sess.run([cross_entropy, accuracy], feed_dict={x:batch_images, y: batch_labels, keep_prob: 1.0, learning_rate: lr})\n",
    "        print(\"step %d, training accuracy %g, loss %g\"%(i, train_accuracy, loss))\n",
    "        accuracy_list.append(train_accuracy)\n",
    "        loss_list.append(loss)\n",
    "        \n",
    "test_imgs, test_labels = get_test_examples(2000)\n",
    "print(\"test accuracy %g\"% accuracy.eval(feed_dict={x: test_imgs, y: test_labels, keep_prob: 1.0, learning_rate: 0.0001}))\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "save_path = saver.save(sess, root_dir + \"/tmp/model.ckpt\")\n",
    "print(\"Model saved in file: %s\" % save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from tensorflow.contrib.session_bundle import exporter\n",
    "\n",
    "tensor_bindings = {\"input\": x,\n",
    "                   \"output\": y,\n",
    "                   \"learning_rate\": learning_rate,\n",
    "                   \"keep_prob\": keep_prob}\n",
    "\n",
    "signatures = { \"classification\": exporter.classification_signature(input_tensor=x, scores_tensor=y),\n",
    "              \"generic\": exporter.generic_signature(tensor_bindings)}\n",
    "export = exporter.Exporter(saver)\n",
    "export.init(sess.graph.as_graph_def(), init_op = tf.initialize_all_variables(), named_graph_signatures=signatures)\n",
    "export.export(root_dir + '/export', tf.constant(50001), sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "saver = tf.train.import_meta_graph(root_dir + '/tmp/model.ckpt.meta')\n",
    "graph = tf.get_default_graph()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    saver.restore(sess, root_dir + '/tmp/model.ckpt')\n",
    "    batch_images, batch_labels = get_test_examples(2000)\n",
    "    loss, train_accuracy = sess.run([cross_entropy, accuracy], feed_dict={x:batch_images, y: batch_labels, keep_prob: 1.0, learning_rate: 1.0e-4})\n",
    "    print(\"step %d, training accuracy %g, loss %g\"%(i, train_accuracy, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_imgs, test_labels = get_test_examples(5000)\n",
    "print(\"test accuracy %g\"% accuracy.eval(feed_dict={x: test_imgs, y: test_labels, keep_prob: 1.0, learning_rate: 0.0001}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(0, 10001, 100), accuracy_list, 'r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver.restore(sess, root_dir + \"/tmp/model.ckpt\")\n",
    "print (\"Model restored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_test_examples(size, start = 0):\n",
    "    labels_ = np.zeros((size+start, 5, 11))\n",
    "    labels_only = {x: [y%10 for y in labels_test_data[x]] for x in labels_test_data}\n",
    "    for x in range(start + size):\n",
    "        for i in range(5-len(labels_only[x])):\n",
    "            labels_[x][i][10] = 1\n",
    "        for i, y in enumerate(labels_only[x]):\n",
    "            labels_[x][i+5-len(labels_only[x])][y] = 1\n",
    "    return test_data[start:start+size], labels_[start:start+size]\n",
    "\n",
    "\n",
    "def decode(output):\n",
    "    result = ''\n",
    "    for x in output:\n",
    "        if x != 10:\n",
    "            result = result + str(x)\n",
    "    return result\n",
    "\n",
    "test_imgs, test_labels = get_test_examples(1, 1)\n",
    "# test_imgs, test_labels = get_train_examples(1)\n",
    "plt.imshow(test_imgs[0], interpolation='nearest')\n",
    "plt.show()\n",
    "pred, acc = sess.run([y_conv, accuracy], feed_dict={x:test_imgs, keep_prob: 1.0, learning_rate: 0.0001, y: batch_labels})\n",
    "print ('accuracy: ', acc)\n",
    "#confidence = reduce(lambda x, y: x*y, [pred[i][x] for i, x in enumerate(np.argmax(pred, 1))])\n",
    "#print ('confidence: ', confidence)\n",
    "pred = tf.reshape(pred, (digits, 11))\n",
    "pred = tf.nn.softmax(pred)\n",
    "print (pred)\n",
    "pred = tf.argmax(pred, 1)\n",
    "print ('prediction : ', pred)\n",
    "print ('correct : ', np.argmax(tf.reshape(test_labels, (digits, 11)).eval(), 1))\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
